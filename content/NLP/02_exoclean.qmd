---
title: "Nettoyer un texte : des exercices pour découvrir l'approche bag-of-words"
draft: false
weight: 20
slug: nlpexo
tags:
  - NLP
  - nltk
  - Littérature
  - preprocessing
  - Exercice
categories:
  - NLP
  - Exercice
type: book
description: |
  Ce chapitre continue de présenter l'approche de __nettoyage de données__ 
  du `NLP` en s'appuyant sur le corpus de trois auteurs
  anglo-saxons : Mary Shelley, Edgar Allan Poe, H.P. Lovecraft.
  Dans cette série d'exercice nous mettons en oeuvre de manière
  plus approfondie les différentes méthodes présentées
  précédemment.
bibliography: ../../reference.bib
image: featured_nlp_exo.png
echo: false
---


La liste des modules à importer est assez longue, la voici :

```{python}
#| output: hide
#| echo: true
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import base64
import string
import re
import nltk

from collections import Counter
from time import time
# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords
from sklearn.metrics import log_loss
import matplotlib.pyplot as plt
#!pip install pywaffle
from pywaffle import Waffle

from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('genesis')
nltk.download('wordnet')
nltk.download('omw-1.4')
```

# Données utilisées




A noter que l'histogramme produit
par le biais de `Matplotlib` ou `Seaborn` est
peu lisible. Il vaut mieux privilégier `Plotly` 
pour faire celui-ci afin d'avoir les mots qui s'affichent en 
passant sa souris sur chaque barre. 


## Aparté : la loi de Zipf

::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> La loi de Zipf</h3>
```
Dans son sens strict, la loi de Zipf prévoit que
dans un texte donné, la fréquence d'occurrence $f(n_i)$ d'un mot est
liée à son rang $n_i$ dans l'ordre des fréquences par une loi de la forme
$f(n_i) = c/n_i$ où $c$ est une constante. Zipf, dans les années 1930, se basait sur l'oeuvre 
de Joyce, *Ulysse* pour cette affirmation. 

Plus généralement, on peut dériver la loi de Zipf d'une distribution exponentielle des fréquences : $f(n_i) = cn_{i}^{-k}$. Cela permet d'utiliser la famille des modèles linéaires généralisés, notamment les régressions poissonniennes, pour mesurer les paramètres de la loi. Les modèles linéaire traditionnels en `log` souffrent en effet, dans ce contexte, de biais (la loi de Zipf est un cas particulier d'un modèle gravitaire, où appliquer des OLS est une mauvaise idée, cf. [@galiana2020segregation](https://linogaliana.netlify.app/publication/2020-segregation/) pour les limites).

```{=html}
</div>
```
:::

Un modèle exponentiel peut se représenter par un modèle de Poisson ou, si 
les données sont très dispersées, par un modèle binomial négatif. Pour
plus d'informations, consulter l'annexe de @galiana2020segregation. 
La technique économétrique associée pour l'estimation est 
les modèles linéaires généralisés (GLM) qu'on peut 
utiliser en `Python` via le
package `statsmodels`[^3]:

[^3]: La littérature sur les modèles gravitaires, présentée dans @galiana2020segregation, 
donne quelques arguments pour privilégier les modèles GLM à des modèles log-linéaires
estimés par moindres carrés ordinaires. 

$$
\mathbb{E}\bigg( f(n_i)|n_i \bigg) = \exp(\beta_0 + \beta_1 \log(n_i))
$$


Prenons les résultats de l'exercice précédent et enrichissons les du rang et de la fréquence d'occurrence d'un mot : 

```{python}
#| echo: true
count_words = pd.DataFrame({'counter' : train
    .groupby('Author')
    .apply(lambda s: ' '.join(s['Text']).split())
    .apply(lambda s: Counter(s))
    .apply(lambda s: s.most_common())
    .explode()}
)
count_words[['word','count']] = pd.DataFrame(count_words['counter'].tolist(), index=count_words.index)
count_words = count_words.reset_index()

count_words = count_words.assign(
    tot_mots_auteur = lambda x: (x.groupby("Author")['count'].transform('sum')),
    freq = lambda x: x['count'] /  x['tot_mots_auteur'],
    rank = lambda x: x.groupby("Author")['count'].transform('rank', ascending = False)
)
```

Commençons par représenter la relation entre la fréquence et le rang:

```{python}
#| output: false
#| echo: true
from plotnine import *

g = (
    ggplot(count_words) +
    geom_point(aes(y = "freq", x = "rank", color = 'Author'), alpha = 0.4) +
    scale_x_log10() + scale_y_log10() +
    theme_minimal()
)
```

Nous avons bien, graphiquement, une relation log-linéaire entre les deux :

```{python}
g
```

Avec `statsmodels`, vérifions plus formellement cette relation:

```{python}
#| echo: true
import statsmodels.api as sm

exog = sm.add_constant(np.log(count_words['rank'].astype(float)))

model = sm.GLM(count_words['freq'].astype(float), exog, family = sm.families.Poisson()).fit()

# Afficher les résultats du modèle
print(model.summary())
```

Le coefficient de la régression est presque 1 ce qui suggère bien une relation
quasiment log-linéaire entre le rang et la fréquence d'occurrence d'un mot. 
Dit autrement, le mot le plus utilisé l'est deux fois plus que le deuxième mot le plus fréquent qui l'est trois plus que le troisième, etc.

# Nettoyage d'un texte

Les premières étapes dans le nettoyage d'un texte, qu'on a
développé au cours du [chapitre précédent](/content/NLP/01_intro.html), sont :

* suppression de la ponctuation ;
* suppression des *stopwords*. 

Cela passe par la tokenisation d'un texte, c'est-à-dire la décomposition
de celui-ci en unités lexicales (les *tokens*).
Ces unités lexicales peuvent être de différentes natures,
selon l'analyse que l'on désire mener.
Ici, on va définir les tokens comme étant les mots utilisés.

Plutôt que de faire soi-même ce travail de nettoyage,
avec des fonctions mal optimisées,
on peut utiliser la librairie `nltk` comme détaillé [précédemment](/content/NLP/01_intro.html). 


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 4 : Nettoyage du texte</h3>
```

Repartir de `train`, notre jeu de données d'entraînement.

1. Tokeniser chaque phrase avec `nltk`.
2. Retirer les stopwords avec `nltk`.


```{=html}
</div>
```
:::

Pour rappel, au début de l'exercice, `train` présente l'aspect suivant :


```{python}
train.head(2)
```

Après tokenisation, il devrait avoir cet aspect :

```{python}
#| output: true

#1. Tokenisation
train_clean = (train
    .groupby(["ID","Author"])
    .apply(lambda s: nltk.word_tokenize(' '.join(s['Text'])))
    .apply(lambda words: [word for word in words if word.isalpha()])
)
train_clean.head(2)
```

Après le retrait des stopwords, cela donnera :

```{python}
#| output: false

#2. Enlever les stopwords.
from nltk.corpus import stopwords  
stop_words = set(stopwords.words('english'))

train_clean = (train_clean
    .apply(lambda words: [w for w in words if not w in stop_words])
    .reset_index(name='tokenized')
)

train_clean.head(2)
```


::: {.cell .markdown}
```{=html}
<div class="alert alert-warning" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-lightbulb"></i> Hint</h3>
```
La méthode `apply` est très pratique ici car nous avons une phrase par ligne. Plutôt que de faire un `DataFrame` par auteur, ce qui n'est pas une approche très flexible, on peut directement appliquer la tokenisation
sur notre `DataFrame` grâce à `apply`, sans le diviser.

```{=html}
</div>
```
:::


Ce petit nettoyage permet d'arriver à un texte plus intéressant en termes d'analyse lexicale. Par exemple, si on reproduit l'analyse précédente... :

```{python}
#| output: false
train_clean["Text"] = train_clean['tokenized'].apply(lambda s: " ".join(map(str, s)))

n_topics = ["HPL","EAP","MWS"]

fig = plt.figure(figsize=(15, 12))
for i in range(len(n_topics)):
    ax = fig.add_subplot(2,2,i+1)
    wordcloud = graph_wordcloud(n_topics[i], train_clean)

    ax.imshow(wordcloud)
    ax.axis('off')

fig
```

```{python}
#| echo: false
fig.get_figure()
```


Pour aller plus loin dans l'harmonisation d'un texte, il est possible de
mettre en place les classes d'équivalence développées dans la 
[partie précédente](/content/NLP/01_intro.html) afin de remplacer différentes variations d'un même
mot par une forme canonique :

* la **racinisation** (*stemming*) assez fruste mais rapide, notamment
en présence de fautes d’orthographe. Dans ce cas, _chevaux_ peut devenir _chev_
mais être ainsi confondu avec _chevet_ ou _cheveux_.
Cette méthode est généralement plus simple à mettre en oeuvre, quoique
plus fruste. 

* la **lemmatisation** qui requiert la connaissance des statuts
grammaticaux (exemple : _chevaux_ devient _cheval_).
Elle est mise en oeuvre, comme toujours avec `nltk`, à travers un
modèle. En l'occurrence, un `WordNetLemmatizer`  (WordNet est une base
lexicographique ouverte). Par exemple, les mots *"women"*, *"daughters"*
et *"leaves"* seront ainsi lemmatisés de la manière suivante :

```{python}
#| echo: true
from nltk.stem import WordNetLemmatizer
lemm = WordNetLemmatizer()

for word in ["women","daughters", "leaves"]:
    print(f"The lemmatized form of {word} is: {lemm.lemmatize(word)}")
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```
Pour disposer du corpus nécessaire à la lemmatisation, il faut, la première fois,
télécharger celui-ci grâce aux commandes suivantes :

~~~python
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
~~~

```{=html}
</div>
```
:::

On va se restreindre au corpus d'Edgar Allan Poe et repartir de la base de données
brute :

```{python}
eap_clean = train[train["Author"] == "EAP"]
eap_clean = ' '.join(eap_clean['Text'])
#Tokenisation naïve sur les espaces entre les mots => on obtient une liste de mots
#tokens = eap_clean.split()
word_list = nltk.word_tokenize(eap_clean)
```


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 5 : Lemmatisation avec nltk</h3>
```

Utiliser un `WordNetLemmatizer` et observer le résultat.

Optionnel : Effectuer la même tâche avec `spaCy`

```{=html}
</div>
```
:::

Le `WordNetLemmatizer` donnera le résultat suivant :

```{python}
#| echo: false

#Exercice 5 : WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])

print(" ".join(word_list[:43]))
print("---------------------------")
print(lemmatized_output[:209])
```

# TF-IDF: calcul de fréquence

Le calcul [tf-idf](https://fr.wikipedia.org/wiki/TF-IDF) (term _frequency–inverse document frequency_)
permet de calculer un score de proximité entre un terme de recherche et un
document (c'est ce que font les moteurs de recherche). 

* La partie `tf` calcule une fonction croissante de la fréquence du terme de recherche dans le document à l'étude ;
* La partie `idf` calcule une fonction inversement proportionnelle à la fréquence du terme dans l'ensemble des documents (ou corpus).

Le score total, obtenu en multipliant les deux composantes,
permet ainsi de donner un score d'autant plus élevé que le terme est surréprésenté dans un document
(par rapport à l'ensemble des documents).
Il existe plusieurs fonctions, qui pénalisent plus ou moins les documents longs,
ou qui sont plus ou moins *smooth*.


::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 6 : TF-IDF: calcul de fréquence</h3>
```

1. Utiliser le vectoriseur TF-IdF de `scikit-learn` pour transformer notre corpus en une matrice `document x terms`. Au passage, utiliser l'option `stop_words` pour ne pas provoquer une inflation de la taille de la matrice. Nommer le modèle `tfidf` et le jeu entraîné `tfs`.
2. Après avoir construit la matrice de documents x terms avec le code suivant, rechercher les lignes où les termes ayant la structure `abandon` sont non-nuls. 
3. Trouver les 50 extraits où le score TF-IDF est le plus élevé et l'auteur associé. Vous devriez obtenir le classement suivant :

```{=html}
</div>
```
:::

```{python}
#| output: false

#1. TfIdf de scikit
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words=stopwords.words("english"))
tfs = tfidf.fit_transform(train['Text'])
```

```{python}
feature_names = tfidf.get_feature_names_out()
corpus_index = [n for n in list(tfidf.vocabulary_.keys())]
import pandas as pd
df = pd.DataFrame(tfs.todense(), columns=feature_names)

df.head()
```

Les lignes où les termes de abandon sont non nuls
sont les suivantes :

```{python}
#| include: true
#| echo: false

#2. Lignes où les termes de abandon sont non nuls.
tempdf = df.loc[(df.filter(regex = "abandon")!=0).any(axis=1)]
print(tempdf.index)
tempdf.head(5)
```


```{python}
#| include: true
#| echo: false

#3. 50 extraits avec le TF-IDF le plus élevé.
list_fear = df["fear"].sort_values(ascending =False).head(n=50).index.tolist()
train.iloc[list_fear].groupby('Author').count()['Text'].sort_values(ascending = False)
```

Les 10 scores les plus élevés sont les suivants :

```{python}
print(train.iloc[list_fear[:9]]['Text'].values)
```

On remarque que les scores les plus élevés sont soient des extraits courts où le mot apparait une seule fois, soit des extraits plus longs où le mot fear apparaît plusieurs fois.



::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

La matrice `document x terms` est un exemple typique de matrice _sparse_ puisque, dans des corpus volumineux, une grande diversité de vocabulaire peut être trouvée.  

```{=html}
</div>
```
:::


# Approche contextuelle : les *n-grams*


Jusqu'à présent, dans l'approche _bag of words_, l'ordre des mots n'avait pas d'importance.
On considère qu'un texte est une collection de
mots tirés indépendamment, de manière plus ou moins fréquente en fonction de leur probabilité
d'occurrence. Cependant, tirer un mot particulier n'affecte pas les chances de tirer certains mots
ensuite, de manière conditionnelle. 

Une manière d'introduire des liens entre les séries de _tokens_ sont les _n-grams_. 
On s'intéresse non seulement aux mots et à leur fréquence, mais aussi aux mots qui suivent. Cette approche est essentielle pour désambiguiser les homonymes. Le calcul de _n-grams_ [^ngrams] constitue la méthode la plus simple pour tenir compte du contexte.

[^ngrams]: On parle de _bigrams_ pour les co-occurences de mots deux-à-deux, _trigrams_ pour les co-occurences trois-à-trois, etc.


Pour être en mesure de mener cette analyse, il est nécessaire de télécharger un corpus supplémentaire :

```{python}
#| echo: true

import nltk
nltk.download('genesis')
nltk.corpus.genesis.words('english-web.txt')
```


`NLTK` offre des methodes pour tenir compte du contexte. Pour ce faire, nous calculons les n-grams, c'est-à-dire l'ensemble des co-occurrences successives de mots n-à-n.  En général, on se contente de bi-grams, au mieux de tri-grams: 

* les modèles de classification, analyse du sentiment, comparaison de documents, etc. qui comparent des n-grams avec n trop grands sont rapidement confrontés au problème de données sparse, cela réduit la capacité prédictive des modèles ;
* les performances décroissent très rapidement en fonction de n, et les coûts de stockage des données augmentent rapidement (environ n fois plus élevé que la base de données initiale).


On va, rapidement, regarder dans quel contexte apparaît le mot `fear` dans
l'oeuvre d'Edgar Allan Poe (EAP). Pour cela, on transforme d'abord
le corpus EAP en tokens `NLTK` : 

```{python}
#| echo: true
eap_clean = train[train["Author"] == "EAP"]
eap_clean = ' '.join(eap_clean['Text'])
tokens = eap_clean.split()
print(tokens[:10])
text = nltk.Text(tokens)
print(text)
```

Vous aurez besoin des fonctions ` BigramCollocationFinder.from_words` et `BigramAssocMeasures.likelihood_ratio` : 

```{python}
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
```

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Exercice 7  : n-grams et contexte du mot fear</h3>
```
1. Utiliser la méthode `concordance` pour afficher le contexte dans lequel apparaît le terme `fear`. 
2. Sélectionner et afficher les meilleures collocations, par exemple selon le critère du ratio de vraisemblance. 

Lorsque deux mots sont fortement associés, cela est parfois dû au fait qu'ils apparaissent rarement. Il est donc parfois nécessaire d'appliquer des filtres, par exemple ignorer les bigrammes qui apparaissent moins de 5 fois dans le corpus.

3. Refaire la question précédente en utilisant toujours un modèle `BigramCollocationFinder` suivi de la méthode `apply_freq_filter` pour ne conserver que les bigrammes présents au moins 5 fois. Puis, au lieu d'utiliser la méthode de maximum de vraisemblance, testez la méthode `nltk.collocations.BigramAssocMeasures().jaccard`.

4. Ne s'intéresser qu'aux *collocations* qui concernent le mot *fear*


```{=html}
</div>
```
:::


Avec la méthode `concordance` (question 1), 
la liste devrait ressembler à celle-ci :

```{python}
#| include: true
#| echo: false

# 1. Methode concordance
print("Exemples d'occurences du terme 'fear' :")
text.concordance("fear")
print('\n')
```

Même si on peut facilement voir le mot avant et après, cette liste est assez difficile à interpréter car elle recoupe beaucoup d'informations. 

La `collocation` consiste à trouver les bi-grammes qui
apparaissent le plus fréquemment ensemble. Parmi toutes les paires de deux mots observées,
il s'agit de sélectionner, à partir d'un modèle statistique, les "meilleures". 
On obtient donc avec cette méthode (question 2):

```{python}
# 2. Modélisation des meilleures collocations
bcf = BigramCollocationFinder.from_words(text)
bcf.nbest(BigramAssocMeasures.likelihood_ratio, 20)
```

Si on modélise les meilleures collocations:

```{python}
# 3. Modélisation des meilleures collocations (qui apparaissent 5+)
finder = nltk.BigramCollocationFinder.from_words(text)
finder.apply_freq_filter(5)
bigram_measures = nltk.collocations.BigramAssocMeasures()
collocations = finder.nbest(bigram_measures.jaccard, 15) 

for collocation in collocations:
    c = ' '.join(collocation)
    print(c)
```

Cette liste a un peu plus de sens,
on a des noms de personnages, de lieux mais aussi des termes fréquemment employés ensemble
(*Chess Player* par exemple).

En ce qui concerne les _collocations_ du mot fear :

```{python}
# 4. collocations du mot fear
bigram_measures = nltk.collocations.BigramAssocMeasures()

def collocations_word(word = "fear"):
    # Ngrams with a specific name 
    name_filter = lambda *w: word not in w
    # Bigrams
    finder = BigramCollocationFinder.from_words(
                nltk.corpus.genesis.words('english-web.txt'))
    # only bigrams that contain 'fear'
    finder.apply_ngram_filter(name_filter)
    # return the 100 n-grams with the highest PMI
    print(finder.nbest(bigram_measures.likelihood_ratio,100))
    
collocations_word("fear")
```

Si on mène la même analyse pour le terme *love*, on remarque que de manière logique, on retrouve bien des sujets généralement accolés au verbe :

```{python}
collocations_word("love")
```


# Références
