



# Nettoyer un texte




Cela montre clairement qu'il est nécessaire de nettoyer notre texte. Le nom
du personnage principal, Dantès, est ainsi masqué par un certain nombre
d'articles ou mots de liaison qui perturbent l'analyse. Ces mots sont des 
*stop words*. 

La librairie `NLTK` (*Natural Language ToolKit*), librairie
de référence dans le domaine du NLP, permet de facilement retirer ces
*stop words* (cela pourrait également être fait avec 
la librairie plus récente, `spaCy`). Avant cela, il est nécessaire
de transformer notre texte en le découpant par unités fondamentales (les _tokens_).

Les exemples suivants, extraits de @galianafuzzy, montrent l'intérêt du
nettoyage de textes lorsqu'on désire comparer des corpus
entre eux. En l'occurrence, il s'agit de comparer un corpus de
noms de produits dans des collectes automatisées de produits 
de supermarché (_scanner-data_) avec des noms de produits
dans les données de l'`OpenFoodFacts`, une base de données
contributive. Sans nettoyage, le bruit l'emporte sur le signal
et il est impossible de déceler des similarités entre les jeux
de données. Le nettoyage permet d'harmoniser
un peu ces jeux de données pour avoir une chance d'être en 
mesure de les comparer. 

::: {layout-ncol=2}
![`OpenFoodFacts` avant nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_openfood_start.png)

![_Scanner-data_ avant nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_relevanc_start.png)
:::

::: {layout-ncol=2}
![`OpenFoodFacts` après nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_openfood_clean.png)

![_Scanner-data_ après nettoyage](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/wordcloud_relevanc_clean.png)
:::



# Reconnaissance des entités nommées

Cette étape n'est pas une étape de préparation mais illustre la capacité 
des librairies `Python` a extraire du sens d'un texte. La librairie 
`spaCy` permet de faire de la reconnaissance d'entités nommées
(_named entity recognition_, NER), ce qui peut
être pratique pour extraire rapidement certains personnages de notre oeuvre.

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> La librairie <code>SpaCy</code></h3>
```

`NTLK` est la librairie historique d'analyse textuelle en `Python`. Elle existe
depuis les années 1990. L'utilisation industrielle du NLP dans le monde
de la _data science_ est néanmoins plus récente et doit beaucoup à la collecte
accrue de données non structurées par les réseaux sociaux. Cela a amené à 
un renouvellement du champ du NLP, tant dans le monde de la recherche que dans
sa mise en application dans l'industrie de la donnée.

Le _package_ [`spaCy`](https://spacy.io/) est l'un des packages qui a permis
cette industrialisation des méthodes de NLP. Conçu autour du concept
de _pipelines_ de données, il est beaucoup plus pratique à mettre en oeuvre
pour une chaîne de traitement de données textuelles mettant en oeuvre
plusieurs étapes de transformation des données. 

```{=html}
</div>
```
:::

Voici un exemple de reconnaissance d'entités nommées
sur les premières phrases de l'ouvrage :

```{python}
#!pip install deplacy
!python -m spacy download fr_core_news_sm
import spacy
import spacy
from spacy import displacy

nlp=spacy.load("fr_core_news_sm")
doc = nlp(dumas[15000:17000])
displacy.render(doc, style="ent", jupyter=True)
```

La reconnaissance d'entités nommées disponible
par défaut est souvent décevante ; il est
souvent nécessaire d'enrichir les règles par défaut
par des règles _ad hoc_, propres à chaque corpus.


