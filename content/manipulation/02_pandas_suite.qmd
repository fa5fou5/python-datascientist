---
title: "Statistiques par groupe et association de plusieurs jeux de données avec Pandas"
draft: false
weight: 20
tags:
  - Pandas
  - Pollution
  - Ademe
  - Tutoriel
  - Manipulation
categories:
  - Tutoriel
  - Manipulation
description: |
  Le chapitre d'introduction à `Pandas` a permis de présenter le principe de données organisées sous une forme de _DataFrame_ et la praticité de l'écosystème `Pandas` pour effectuer des opérations simples sur un jeu de données. Ce chapitre consolide ces principes en présentant deux types de traitements classiques de la boite à outil des _data scientists_ : statistiques par groupe et associations de données. 
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/panda_stretching.png
links:
- icon: journal-text
  name: Documentation Pandas
  url: https://pandas.pydata.org/docs/
---

# Introduction

Le [chapitre d'introduction à `Pandas`](/content/manipulation/02_pandas_intro.qmd) a permis de présenter le principe de données organisées sous une forme de _DataFrame_ et la praticité de l'écosystème `Pandas` pour effectuer des opérations simples sur un jeu de données.

Il est rare de travailler exclusivement sur une source brute. Un jeu de données prend généralement de la valeur lorsqu'il est comparé à d'autres sources. Pour des chercheurs, cela permettra de contextualiser l'information présente dans une source en la comparant ou en l'associant à d'autres sources. Pour des _data scientists_ dans le secteur privé, il s'agira souvent d'associer des informations sur une même personne dans plusieurs bases clientes ou comparer les clients entre eux.

L'un des apports des outils modernes de _data science_, notamment `Pandas` est la simplicité par laquelle ils permettent de restructurer des sources pour travailler sur plusieurs données sur un projet. 
Ce chapitre consolide ainsi les principes vus précédemment en raffinant les traitements faits sur les données. Il va explorer principalement deux types d'opérations:

- les statistiques descriptives par groupe ;
- l'association de données par des caractéristiques communes.

Effectuer ce travail de manière simple, fiable et efficace est indispensable pour les _data scientists_ tant cette tâche est courante. Heureusement `Pandas` permet de faire cela très bien avec des données structurées. Nous verrons dans les prochains chapitres, mais aussi dans l'ensemble de la [partie sur le traitement des données textuelles](/content/nlp/index.qmd), comment faire avec des données moins structurées.

::: {.cell .markdown}
```{=html}
<div class="alert alert-success" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-pencil"></i> Compétences à l'issue de ce chapitre</h3>
```

- Récupérer un jeu de données officiel de l'Insee ;
- Construire des statistiques descriptives par groupe et jongler entre les niveaux des données ;
- Associer des données (_reshape_, _merge_) pour leur donner plus de valeur ;
- Faire un beau tableau pour communiquer des statistiques descriptives.
```{=html}
</div>
```
:::


## Environnement

Le chapitre précédent utilisait quasi exclusivement la librairie `Pandas`. Nous allons dans ce chapitre utiliser d'autres _packages_ en complément de celui-ci. 

Comme expliqué ci-dessous, nous allons utiliser une librairie nommée `pynsee` pour récupérer les données de l'Insee utiles à enrichir notre jeu de données de l'Ademe. Cette librairie n'est pas installée par défaut dans `Python`. Avant de pouvoir l'utiliser,
il est nécessaire de l'installer, comme la librairie `great_tables` que nous verrons à la fin de ce chapitre:

```{python}
#| eval: false
#| echo: true
!pip install xlrd
!pip install pynsee
!pip install great_tables
```

L'instruction `!pip install <pkg>` est une manière de faire comprendre à `Jupyter`, le moteur d'exécution derrière les _notebooks_ que la commande qui suit (`pip install` ce `<pkg>`)
est une commande système, à exécuter hors de `Python` (dans le terminal par exemple pour un système `Linux`).  

Les premiers _packages_ indispensables pour démarrer ce chapitre sont les suivants:

```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pynsee
import pynsee.download
```

Pour obtenir des résultats reproductibles, on peut fixer la racine du générateur
pseudo-aléatoire. 

```{python}
#| echo: true
np.random.seed(123)
```


## Données utilisées

Ce tutoriel continue l'exploration du jeu de données du chapitre précédent:

* Les émissions de gaz à effet de serre estimées au niveau communal par l'ADEME. Le jeu de données est 
disponible sur [data.gouv](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_)
et requêtable directement dans `Python` avec
[cet url](https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert) ;


Les problématiques d'enrichissement de données (association d'une source à une autre à partir de caractéristiques communes) seront présentées à partir de deux sources produites par l'Insee:

* Le 
[code officiel géographique](https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv),
un référentiel
produit par l'Insee utilisé pour identifier les communes à partir d'un code unique, contrairement au code postal ;
* Les données [_Filosofi_](https://www.insee.fr/fr/metadonnees/source/serie/s1172), une source sur les revenus des Français à une échelle spatiale fine construite par l'Insee à partir des déclarations fiscales et d'informations sur les prestations sociales. En l'occurrence, nous allons utiliser les niveaux de revenu et les populations[^poplegales] au niveau communal afin de les mettre en regard de nos données d'émissions.


[^poplegales]: Idéalement il serait plus cohérent, pour les données démographiques, d'utiliser les [populations légales](https://www.insee.fr/fr/information/2008354), issues du recensement. Néanmoins cette base n'est pas encore intégrée nativement dans la librairie `pynsee` que nous allons utiliser dans ce chapitre. Un exercice d'ouverture est proposé pour construire des agrégats de population à partir des jeux de données individuels anonymisés du recensement (les [fichiers détails](https://www.insee.fr/fr/information/2383306)).


Pour faciliter l'import de données Insee, il est recommandé d'utiliser le _package_
[`pynsee`](https://pynsee.readthedocs.io/en/latest/) qui simplifie l'accès aux principaux jeux de données
de l'Insee disponibles sur le site web [insee.fr](https://www.insee.fr/fr/accueil)
ou via des API. 

::: {.cell .markdown}
```{=html}
<div class="alert alert-info" role="alert">
<h3 class="alert-heading"><i class="fa-solid fa-comment"></i> Note</h3>
```

Le _package_ `pynsee` comporte deux principaux points d'entrée :

- Les API de l'Insee, ce qui sera illustré dans le chapitre consacré.
- Quelques jeux de données directement issus du site web de
l'Insee ([insee.fr](https://www.insee.fr/fr/accueil))

Dans ce chapitre, nous allons exclusivement utiliser cette deuxième
approche. Cela se fera par le module `pynsee.download`.  

La liste des données disponibles depuis ce _package_ est [ici](https://inseefrlab.github.io/DoReMIFaSol/articles/donnees_dispo.html).
La fonction `download_file` attend un identifiant unique
pour savoir quelle base de données aller chercher et
restructurer depuis le
site [insee.fr](https://www.insee.fr/fr/accueil). 

<details>
<summary>
Connaître la liste des bases disponibles
</summary>

Pour connaître la liste des bases disponibles, vous
pouvez utiliser la fonction `meta = pynsee.get_file_list()`
après avoir fait `import pynsee`. 
Celle-ci renvoie un `DataFrame` dans lequel on peut
rechercher, par exemple grâce à une recherche
de mots-clefs : 

```{python}
#| echo: true
import pynsee
meta = pynsee.get_file_list()
meta.loc[meta['label'].str.contains(r"Filosofi.*2016")]
```

Ici, `meta['label'].str.contains(r"Filosofi.*2016")` signifie:
"_`pandas` trouve moi tous les labels où sont contenus les termes Filosofi et 2016._"
 (`.*` signifiant "_peu m'importe le nombre de mots ou caractères entre_")

</details>

```{=html}
</div>
```
:::



# Récupération des jeux de données

## Données d'émission de l'Ademe

Comme expliqué au chapitre précédent, ces données peuvent être importées très simplement avec `Pandas`

```{python}
#| echo: true
import pandas as pd

emissions = pd.read_csv("https://koumoul.com/s/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/convert")
emissions.head(2)
```

Les exploitations ultérieures de ces données utiliseront la dimension départementale dont nous avons montré la construction au chapitre précédent:

```{python}
emissions['dep'] = emissions["INSEE commune"].str[:2]
```


## Données _Filosofi_


On va utiliser les données Filosofi (données de revenus) au niveau communal de 2016. 
Ce n'est pas la même année que les données d'émission de CO2, ce n'est donc pas parfaitement rigoureux,
mais cela permettra tout de même d'illustrer 
les principales fonctionnalités de `Pandas`

Le point d'entrée principal de la fonction `pynsee` est la fonction `download_file`.

Le code pour télécharger les données est le suivant :

```{python}
#| echo: true
#| output: false
from pynsee.download import download_file
filosofi = download_file("FILOSOFI_COM_2016")
```

Le _DataFrame_ en question a l'aspect suivant :

```{python}
filosofi.sample(3)
```

`Pandas` a géré automatiquement les types de variables. Il le fait relativement bien, mais une vérification est toujours utile pour les variables qui ont un statut spécifique.

Pour les variables qui ne sont pas en type `float` alors qu’elles devraient l’être, on modifie leur type.

```{python}
#| echo: true
filosofi.loc[:, filosofi.columns[2:]] = (
  filosofi.loc[:, filosofi.columns[2:]]
  .apply(pd.to_numeric, errors='coerce')
)
```

Un simple coup d'oeil sur les données
donne une idée assez précise de la manière dont les données sont organisées.
On remarque que certaines variables de `filosofi` semblent avoir beaucoup de valeurs manquantes (secret statistique)
alors que d'autres semblent complètes. 
Si on désire exploiter `filosofi`, il faut faire attention à la variable choisie.


Notre objectif à terme va être de relier l'information contenue entre ces
deux jeux de données. En effet, sinon, nous risquons d'être frustré : nous allons
vouloir en savoir plus sur les émissions de gaz carbonique mais seront très
limités dans les possibilités d'analyse sans ajout d'une information annexe
issue de `filosofi`.  


# Statistiques descriptives par groupe

## Principe

Nous avons vu, lors du chapitre précédent, comment obtenir
une statistique agrégée simplement grâce à `Pandas`. 
Il est néanmoins commun d'avoir des données avec des strates
intermédiaires d'analyse pertinentes: des variables géographiques, l'appartenance à des groupes socio-démographiques liés à des caractéristiques renseignées, des indicatrices de période temporelle, etc. 
Pour mieux comprendre la structure de ses données, les _data scientists_ sont donc souvent amenés à construire des statistiques descriptives sur des sous-groupes présents dans les données. Pour reprendre l'exemple sur les émissions, nous avions précédemment construit des statistiques d'émissions au niveau national. Mais qu'en est-il du profil d'émission des différents départements ? Pour répondre à cette question, il sera utile d'agréger nos données au niveau départemental. Ceci nous donnera une information différente du jeu de données initial (niveau communal) et du niveau le plus agrégé (niveau national).

En `SQL`, il est très simple de découper des données pour
effectuer des opérations sur des blocs cohérents et recollecter des résultats
dans la dimension appropriée.
La logique sous-jacente est celle du *split-apply-combine* qui est repris
par les langages de manipulation de données, auxquels `pandas`
[ne fait pas exception](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html). 

L'image suivante, issue de
[ce site](https://unlhcc.github.io/r-novice-gapminder/16-plyr/),
représente bien la manière dont fonctionne l'approche
`split`-`apply`-`combine`:

![Split-apply-combine (Source: [unlhcc.github.io](https://unlhcc.github.io/r-novice-gapminder/16-plyr/))](https://unlhcc.github.io/r-novice-gapminder/fig/12-plyr-fig1.png){fig-width=70%}

En `Pandas`, on utilise `groupby` pour découper les données selon un ou
plusieurs axes (ce [tutoriel](https://realpython.com/pandas-groupby/) sur le sujet
est particulièrement utile).
L'ensemble des opérations d'agrégation (comptage, moyennes, etc.) que nous avions vues précédemment peut être mise en oeuvre par groupe.

Techniquement, cette opération consiste à créer une association
entre des labels (valeurs des variables de groupe) et des
observations. Utiliser la méthode `groupby` ne déclenche pas d'opérations avant la mise en oeuvre d'une statistique, cela créé seulement une relation formelle entre des observations et des regroupemens qui seront utilisés _a posteriori_

```{python}
#| echo: true
filosofi["dep"] = filosofi["CODGEO"].str[:2]
filosofi.groupby('dep').__class__
```


## Illustration 1: dénombrement p

Pour illustrer l'application de ce principe à un comptage, on peut dénombrer le nombre de communes par département en 2023 (chaque année cette statistique change du fait des fusions de communes). Pour cela, il suffit de prendre le référentiel des communes françaises issu du code officiel géographique (COG) et dénombrer par département grâce à `count`:

```{python}
#| echo: true
import pandas as pd

url_cog_2023 = "https://www.insee.fr/fr/statistiques/fichier/6800675/v_commune_2023.csv"
cog_2023 = pd.read_csv(url_cog_2023) 
```

Grâce à ce jeu de données, sans avoir recours aux statistiques par groupe, on peut déjà savoir combien on a, respectivement, de communes, départements et régions en France:

```{python}
#| echo: true
communes = cog_2023.loc[cog_2023['TYPECOM']=="COM"] #<1>
communes.loc[:, ['COM', 'DEP', 'REG']].nunique()
```
1. On se restreint au statut "Commune" car ce fichier comporte également les codes Insee pour d'autres status, comme les "Arrondissements municipaux" de Paris, Lyon et Marseille.  

Maintenant, intéressons nous aux départements ayant le plus de communes. Il s'agit de la même fonction de dénombrement où on joue, cette fois, sur le groupe à partir duquel est calculé la stratégie. 

Calculer cette statistique se fait de manière assez transparente lorsqu'on connaît le principe d'un calcul de statistiques avec `Pandas`:

```{python}
#| echo: true
communes = cog_2023.loc[cog_2023['TYPECOM']=="COM"] #<1>
communes.groupby('DEP').agg({'COM': 'nunique'})
```

La sortie est une `Serie` indexée. Ce n'est pas très pratique comme nous avons pu l'évoquer au cours du chapitre précédent. Il est plus pratique de transformer cet objet en `DataFrame` avec `reset_index`. Enfin, avec `sort_values`, on obtient la statistique désirée:

```{python}
#| echo: true
(
    communes
    .groupby('DEP')
    .agg({'COM': 'nunique'})
    .reset_index()
    .sort_values('COM', ascending = False)
)
```

## Illustration 2: agrégat par groupe

