---
title: "Introduction à Pandas"
draft: false
weight: 20
tags:
  - Pandas
  - Pollution
  - Ademe
  - Tutoriel
  - Manipulation
categories:
  - Tutoriel
  - Manipulation
aliases: ../pandas.html
description: |
  `Pandas` est l'élément central de l'écosystème `Python` pour la _data science_. 
  Le succès récent de `Python` dans l'analyse de données tient beaucoup à `Pandas` qui a permis d'importer la
  logique `SQL` dans le langage `Python`. `Pandas` embarque énormément de
  fonctionalités qui permettent d'avoir des chaînes de traitement efficaces pour
  traiter des données de volumétrie moyenne (jusqu'à quelques Gigas). Au-delà
  de cette volumétrie, il faudra se tourner vers d'autres solutions
  (`DuckDB`, `Dask`, `Polars`, `Spark`...).
bibliography: ../../reference.bib
image: https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/panda_stretching.png
links:
- icon: journal-text
  name: Documentation Pandas
  url: https://pandas.pydata.org/docs/
---



## Opérations par groupe

En `SQL`, il est très simple de découper des données pour
effectuer des opérations sur des blocs cohérents et recollecter des résultats
dans la dimension appropriée.
La logique sous-jacente est celle du *split-apply-combine* qui est repris
par les langages de manipulation de données, auxquels `pandas`
[ne fait pas exception](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html).

L'image suivante, issue de
[ce site](https://unlhcc.github.io/r-novice-gapminder/16-plyr/)
représente bien la manière dont fonctionne l'approche
`split`-`apply`-`combine`

![Split-apply-combine](https://unlhcc.github.io/r-novice-gapminder/fig/12-plyr-fig1.png){fig-width=70%}


Ce [tutoriel](https://realpython.com/pandas-groupby/) sur le sujet
est particulièrement utile.

Pour donner quelques exemples, on peut créer une variable départementale qui
servira de critère de groupe.

```{python}
df['dep'] = df['INSEE commune'].str[:2]
```

En `pandas`, on utilise `groupby` pour découper les données selon un ou
plusieurs axes. Techniquement, cette opération consiste à créer une association
entre des labels (valeurs des variables de groupe) et des
observations.

Par exemple, pour compter le nombre de communes par département en SQL, on
utiliserait la requête suivante :

```sql
SELECT dep, count(INSEE commune)
FROM df
GROUP BY dep;
```

Ce qui, en `pandas`, donne:

```{python}
df.groupby('dep')["INSEE commune"].count()
```

La syntaxe est quasiment transparente. On peut bien sûr effectuer des opérations
par groupe sur plusieurs colonnes. Par exemple,

```{python}
df.groupby('dep').mean(numeric_only = True)
```

A noter que la variable de groupe, ici `dep`, devient, par défaut, l'index
du DataFrame de sortie. Si on avait utilisé plusieurs variables de groupe,
on obtiendrait un objet multi-indexé. Sur la gestion des `multi-index`, on
pourra se référer à l'ouvrage `Modern Pandas` dont la référence est
donnée en fin de cours.

Tant qu'on n'appelle pas une action sur un `DataFrame` par groupe, du type
`head` ou `display`, `pandas` n'effectue aucune opération. On parle de
*lazy evaluation*. Par exemple, le résultat de `df.groupby('dep')` est
une transformation qui n'est pas encore évaluée :

```{python}
df.groupby('dep')
```

Il est possible d'appliquer plus d'une opération à la fois grâce à la méthode
`agg`. Par exemple, pour obtenir à la fois le minimum, la médiane et le maximum
de chaque département, on peut faire:

```{python}
numeric_columns = df.select_dtypes(['number']).columns
df.loc[:, numeric_columns.tolist() + ["dep"] ].groupby('dep').agg(['min',"median","max"], numeric_only = True)
```

La première ligne est présente pour nous faciliter la récupération des noms de colonnes des variables
numériques

## Appliquer des fonctions

`pandas` est, comme on a pu le voir, un package très flexible, qui
propose une grande variété de méthodes optimisées. Cependant, il est fréquent
d'avoir besoin de méthodes non implémentées.

Dans ce cas, on recourt souvent aux `lambda` functions. Par exemple, si
on désire connaître les communes dont le nom fait plus de 40 caractères,
on peut appliquer la fonction `len` de manière itérative:

```{python}
# Noms de communes superieurs à 40 caracteres
df[df['Commune'].apply(lambda s: len(s)>40)]
```


Cependant, toutes les `lambda` functions ne se justifient pas.
Par exemple, prenons
le résultat d'agrégation précédent. Imaginons qu'on désire avoir les résultats
en milliers de tonnes. Dans ce cas, le premier réflexe est d'utiliser
la `lambda` function suivante :

```{python}
numeric_columns = df.select_dtypes(['number']).columns
(df
    .loc[:, numeric_columns.tolist() + ["dep"] ]
    .groupby('dep')
    .agg(['min',"median","max"])
    .apply(lambda s: s/1000)
)
```

En effet, cela effectue le résultat désiré. Cependant, il y a mieux : utiliser
la méthode `div`:

```{python}
#| eval: false
import timeit
df_numeric = df.loc[:, numeric_columns.tolist() + ["dep"] ]
%timeit df_numeric.groupby('dep').agg(['min',"median","max"]).div(1000)
%timeit df_numeric.groupby('dep').agg(['min',"median","max"]).apply(lambda s: s/1000)
```

La méthode `div` est en moyenne plus rapide et a un temps d'exécution
moins variable. Dans ce cas, on pourrait même utiliser le principe
du *broadcasting* de numpy (cf. [chapitre numpy](numpy)) qui offre
des performances équivalentes:


```{python}
#| eval: false
%timeit df_numeric.groupby('dep').agg(['min',"median","max"])/1000
```

`apply` est plus rapide qu'une boucle (en interne, `apply` utilise `Cython`
pour itérer) mais reste moins rapide qu'une solution vectorisée quand
elle existe. Ce [site](https://realpython.com/fast-flexible-pandas/#pandas-apply)
propose des solutions, par exemple les méthodes `isin` ou `digitize`, pour
éviter de manuellement créer des boucles lentes.

En particulier, il faut noter que `apply` avec le paramètre `axis=1` est en générale lente.

# Joindre des données

Il est commun de devoir combiner des données issues de sources différentes.
Nous allons ici nous focaliser sur le cas le plus favorable qui est la situation
où une information permet d'apparier de manière exacte deux bases de données (autrement nous
serions dans une situation, beaucoup plus complexe, d'appariement flou[^6]).

[^6]: Sur l'appariement flou, se reporter aux chapitres présentant `ElasticSearch`. 

La situation typique est l'appariement entre deux sources de données selon un identifiant
individuel. Ici, il s'agit d'un identifiant de code commune.

Il est recommandé de lire [ce guide assez complet sur la question des jointures avec `R`](https://www.book.utilitr.org/03_fiches_thematiques/fiche_joindre_donnees)
qui donne des recommandations également utiles pour un utilisateur de `Python`.

![](https://external-preview.redd.it/yOLzCR0qSzul2WpjQorxINB0xpU3_N9twmFVsgbGJwQ.jpg?auto=webp&s=4feedc91302ba635b3028a21b98d047def5cdc2b)

On utilise de manière indifférente les termes *merge* ou *join*.
Le deuxième terme provient de la syntaxe SQL.
En `Pandas`, dans la plupart des cas, on peut utiliser indifféremment `df.join` et `df.merge`

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/pandas_join.png)

Il est aussi possible de réaliser un merge en utilisant la fonction `pandas.concat()` avec `axis=1`.
Se référer à la documentation de `concat` pour voir les options possibles.

# Restructurer des données (_reshape_)

On présente généralement deux types de données :

* format __wide__ : les données comportent des observations répétées, pour un même individu (ou groupe), dans des colonnes différentes
* format __long__ : les données comportent des observations répétées, pour un même individu, dans des lignes différentes avec une colonne permettant de distinguer les niveaux d'observations

Un exemple de la distinction entre les deux peut être emprunté à l'ouvrage de référence d'Hadley Wickham, *R for Data Science* :

![](https://d33wubrfki0l68.cloudfront.net/3aea19108d39606bbe49981acda07696c0c7fcd8/2de65/images/tidy-9.png)


L'aide mémoire suivante aidera à se rappeler les fonctions à appliquer si besoin :

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/reshape.png)

Le fait de passer d'un format *wide* au format *long* (ou vice-versa) peut être extrêmement pratique car
certaines fonctions sont plus adéquates sur une forme de données ou sur l'autre.
En règle générale, avec `Python` comme avec `R`, les formats *long* sont souvent préférables.

Le chapitre suivant, qui fait office de TP, proposera des applications de ces principes :

::: {.cell .markdown}
```{python}
#| echo: false
#| output: 'asis'
#| include: true
#| eval: true

import sys
sys.path.insert(1, '../../') #insert the utils module
from utils import print_badges

#print_badges(__file__)
print_badges("content/manipulation/02b_pandas_TP.qmd")
```
:::

# `Pandas` dans une chaîne d'opérations

## Les _pipe_

En général, dans un projet, le nettoyage de données va consister en un ensemble de
méthodes appliquées à un `pandas.DataFrame`.
On a vu que `assign` permettait de créer une variable dans un `DataFrame`.
Il est également possible d'appliquer une fonction, appelée par exemple `my_udf` au
DataFrame grâce à `pipe`:

```python
df = (pd.read_csv(path2data)
            .pipe(my_udf))
```

L'utilisation des `pipe` rend le code très lisible et peut être très
pratique lorsqu'on enchaine des opérations sur le même
_dataset_. 


## Quelques enjeux de performance

La librairie `Dask` intègre la structure de `numpy`, `pandas` et `sklearn`.
Elle a vocation à traiter de données en grande dimension, ainsi elle ne sera pas
optimale pour des données qui tiennent très bien en RAM.
Il s'agit d'une librairie construite sur la parallélisation.
[Un chapitre dans ce cours](/dask.html) lui est consacré.
Pour aller plus loin, se référer à la [documentation de `Dask`](https://docs.dask.org/en/latest/).


# Références {.unnumbered}

* Le site
[pandas.pydata](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html)
fait office de référence

* Le livre `Modern Pandas` de Tom Augspurger : https://tomaugspurger.github.io/modern-1-intro.html

::: {#refs}
:::

